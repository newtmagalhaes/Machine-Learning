{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gerando histograma normalizado\n",
    "\n",
    "- normalizando pela quantidade de pixeis da imagem;\n",
    "- normalizando pelo maior elemento do histograma;\n",
    "\n",
    "## Importando bibliotecas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Parâmetros do LBP\n",
    "METHOD = 'nri_uniform'\n",
    "RADIUS = 1\n",
    "N_POINTS = 8\n",
    "\n",
    "TARGET_NAMES = {'nao_faixa': 0,\n",
    "                'faixa_frente': 1,\n",
    "                'faixa_diagonal_direita': 2,\n",
    "                'faixa_diagonal_esquerda': 3}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processando imagens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path_dict = {name: [] for name in TARGET_NAMES}\n",
    "\n",
    "with ZipFile('./texturas.zip') as zip_file:\n",
    "  for path in zip_file.namelist():\n",
    "    # a pasta de cada amostra é o segundo elemento do split\n",
    "    folder = path.split('/')[1]\n",
    "    # Separando amostras por classe\n",
    "    if folder in TARGET_NAMES:\n",
    "      path_dict[folder].append(f'./texturas.zip/{path}')\n",
    "\n",
    "# Mostrando quantidade de amostras por classe\n",
    "for target in path_dict:\n",
    "  print(f'{target}: {len(path_dict[target])} amostras')\n",
    "\n",
    "amostra_n_pixels = []\n",
    "amostra_max_elem = []\n",
    "limiares = {name:[] for name in TARGET_NAMES}\n",
    "\n",
    "for target_name in TARGET_NAMES:\n",
    "    for path in path_dict[target_name]:\n",
    "        img = io.imread(path, as_gray=True)\n",
    "\n",
    "        # Binarizando imagem com limiar de OTSU\n",
    "        img_otsu = img >= threshold_otsu(img)\n",
    "        \n",
    "        lbp = local_binary_pattern(image=img_otsu, P=N_POINTS, R=RADIUS, method=METHOD).flatten()\n",
    "        hist = pd.Series(lbp).value_counts().sort_index()\n",
    "        arr = np.zeros(59)\n",
    "        for i in hist.index:\n",
    "            arr[int(i)] = hist[i]\n",
    "\n",
    "        # Normalizando pela quantidade de píxels da imagem\n",
    "        norm_n_pixels = arr / arr.sum()\n",
    "\n",
    "        df1 = pd.DataFrame(data=[norm_n_pixels])\n",
    "        df1['target'] = TARGET_NAMES[target_name]\n",
    "        amostra_n_pixels.append(df1)\n",
    "\n",
    "        # Normalizando pelo maior elemento do histograma\n",
    "        norm_max_elem = arr / arr.max()\n",
    "\n",
    "        df2 = pd.DataFrame(data=[norm_max_elem])\n",
    "        df2['target'] = TARGET_NAMES[target_name]\n",
    "        amostra_max_elem.append(df2)\n",
    "\n",
    "\n",
    "data_n_pixels = pd.concat(amostra_n_pixels, ignore_index=True)\n",
    "data_n_pixels.to_csv('./csvs/otsu-norm_n_pixels-lbp_faixas.csv')\n",
    "\n",
    "data_max_elem = pd.concat(amostra_max_elem, ignore_index=True)\n",
    "data_max_elem.to_csv('./csvs/otsu-norm_max_elem-lbp_faixas.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nao_faixa: 750 amostras\n",
      "faixa_frente: 250 amostras\n",
      "faixa_diagonal_direita: 250 amostras\n",
      "faixa_diagonal_esquerda: 250 amostras\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}